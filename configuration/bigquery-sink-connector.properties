name=bigquery-sink-connector

connector.class= com.wepay.kafka.connect.bigquery.BigQuerySinkConnector

# Specify if value converter contains a schema
value.converter.schemas.enable=false

# This is the converter for the value 
value.converter=org.apache.kafka.connect.json.JsonConverter

# This is the converter for the key
key.converter=org.apache.kafka.connect.storage.StringConverter

topics=security-events

# The name of the BigQuery project to write to
project=pandaq-404414

# The name of the BigQuery data set to write to (leave the .*= at the beginning and enter your
# data set after it)
datasets=.*=security_analytics_data

# The location of a BigQuery service account or user JSON credentials file
# or service account credentials or user credentials in JSON format (non-escaped JSON blob)
defaultDataset=security_analytics_data

keyfile=<PATH OF YOUR BIGQUERY KEY FILE>

# 'FILE' if keyfile is a credentials file, 'JSON' if it's a credentials JSON
keySource=FILE

# Whether to automatically sanitize topic names before using them as table names
# If not enabled, topic names will be used directly as table names
sanitizeTopics=false

# Automatically create BigQuery tables if they don't already exist
autoCreateTables=false

# Whether or not to automatically update BigQuery schemas
autoUpdateSchemas=false

schemaRetriever=com.wepay.kafka.connect.bigquery.retrieve.IdentitySchemaRetriever

# The maximum number of records to buffer per table before temporarily halting the flow of new records, or -1 for unlimited buffering
bufferSize=100

maxWriteSize=100

tableWriteWait=1000

timestamp=UTC

bigQueryPartitionDecorator=false
